{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244143af",
   "metadata": {},
   "source": [
    "# <font color= Purple> 0. Problem statement: Telecom Churn Group study\n",
    "    \n",
    "                                                                                   Team- 1. Varun Shenoy\n",
    "                                                                                         2. Binay Yadab\n",
    "\n",
    "In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n",
    "\n",
    "For many incumbent operators, retaining high profitable customers is the number one business\n",
    "goal. To reduce customer churn, telecom companies need to predict which customers are at high risk of churn. In this project, we should analyze customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn, and identify the main indicators of churn.\n",
    "\n",
    "### Objective   \n",
    "The goal is to build a machine learning model that is able to predict churning customers based on the features provided for their usage.\n",
    "\n",
    "- Identify customers at high risk of churn by building a predicitve ML model\n",
    "- To Identify important churn predictors\n",
    "- Improve the overall accuracy of the model, using different models and explain the business objectives\n",
    "- Recommend different strategies to cointain the churn based on observations from models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae28717",
   "metadata": {},
   "source": [
    "**The Data**\n",
    "\n",
    "Training data in a CSV file along with metadata is provided. The data has 172 columns highlighting the customer behavior, usage, payment, and other patterns that might be relevant. The target variable is \"churn_probability\".\n",
    "\n",
    "Steps followed in solving the Telecom churn case study\n",
    "\n",
    "<b>Step-0 Understanding problem</b> \n",
    "\n",
    "<b>Step-1 Data Understanding (EDA) and visualization:</b> Impute missing value, null value treatment, Univariate and Bivariate analysis, visulaizing the data with appropriate plots\n",
    "\n",
    "<b>Step-2 Data Preperation and modeling:</b> Class imbalance, dummy variables, and scaling, train test split, build different models like, LR, decision Tree classifier, random forest etc\n",
    "\n",
    "<b>Step-3 Model Development and Evaluation:</b> identify the important features and best model.\n",
    "\n",
    "<b>Step-4 Prediction:</b> Predictions on unseen test data  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfda141b",
   "metadata": {},
   "source": [
    "## <font color= Purple> Step-1 Data Understanding (EDA) and visualization:\n",
    "## <font color= Blue> 1.1 Loading dependencies & datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2601fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Structures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "# To hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',10000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# ML libreries of Sklearn and stats model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco = pd.read_csv('train.csv')\n",
    "print(\"Telecom training data :\",len(df_telco))\n",
    "df_telco.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c9fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b6db6",
   "metadata": {},
   "source": [
    "## <font color= Blue> 1.2 Understanding the Data (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data description\n",
    "df_telco.describe(percentiles=[0.25, 0.50, 0.75, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51634fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check for row repetation\n",
    "df_telco.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41281900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capturing rows eith single entries\n",
    "j=0\n",
    "single_entry_list=[]\n",
    "for i in df_telco.columns:\n",
    "    if df_telco[i].nunique() <= 10:\n",
    "        j+=1\n",
    "        print('\\n', df_telco[i].value_counts(),sep=\"\")\n",
    "        if df_telco[i].nunique()==1:\n",
    "            single_entry_list.append(i)\n",
    "print (\"Total columns with less than 11 entries:\",j)\n",
    "print (single_entry_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fdb734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to understand meaing of terms\n",
    "df_data_dict = pd.read_csv(\"data_dictionary.csv\")\n",
    "df_data_dict.style #to view all text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0698426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Missing value in all columns\n",
    "round(df_telco.isnull().sum()/len(df_telco),4)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e7954",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "1. training data has 172 columns and 69999 rows\n",
    "2. all data is the numeric formats except date representations which is in MM/DD/YYYY format\n",
    "3. There are outliers present in many columns as seen from describe but they need to be analysed further\n",
    "4. There are no duplicate rows observed in the dataset\n",
    "5. There are 24 columns with less than 11 entries with many having single entry in them.\n",
    "6. Many null values are present in columns A pattern is seen here there are either missing values  under 6% or above 70% in different columns. Further study needed to impute or remove them.\n",
    "7. There is a ID column which is not needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed284c55",
   "metadata": {},
   "source": [
    "## <font color= Blue> 1.3 Missing value treatment (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9065d0",
   "metadata": {},
   "source": [
    "Dropping values with missing values greater than 60% as they cannot be imputed and would leading to skewing of data if imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94593436",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_val_60_list=df_telco.columns[100*(df_telco.isnull().sum()/len(df_telco)) > 60]\n",
    "print(\"Number of columns with more than 60% missing values = \",len(missing_val_60_list))\n",
    "print(missing_val_60_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328c0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns from the data set for dropping\n",
    "drop_list=list(missing_val_60_list)+single_entry_list\n",
    "drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping with id\n",
    "df_telco_v1 = df_telco.drop(drop_list,axis=1)\n",
    "df_telco_v1 = df_telco_v1.drop('id',axis=1)\n",
    "print(\"Telecom dataset before dropping columns:\",df_telco.shape)\n",
    "print(\"Telecom dataset after dropping columns:\",df_telco_v1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab083f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_telco_v1.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752963ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfd97cb",
   "metadata": {},
   "source": [
    "Dates can be dropped as we have monthly reacharge and their amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capturing date columns\n",
    "date_drop=[]\n",
    "for i in df_telco_v1.columns:\n",
    "    if df_telco_v1[i].dtype=='object':\n",
    "        date_drop.append(i)\n",
    "\n",
    "df_telco_v1 = df_telco_v1.drop(date_drop,axis=1)\n",
    "df_telco_v1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39821f16",
   "metadata": {},
   "source": [
    "## <font color= Blue> 1.4 Null value imputaion (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ba63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting columns with null values\n",
    "cols_w_null = df_telco_v1.columns[100*(df_telco_v1.isnull().sum()/len(df_telco_v1)) > 0]\n",
    "print(\"Total Columns with missing values in it = \",len(cols_w_null))\n",
    "print(cols_w_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8eb1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding the distribution for columns with null\n",
    "plt.figure(figsize=(35, 100))\n",
    "for i in range (0,len(cols_w_null)):\n",
    "    plt.subplot(17,5,i+1)\n",
    "    grp= sns.distplot(df_telco_v1[cols_w_null[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae183cb",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "1. we see that most of the variables have vast distribution but most of their values are zero or close within 100.\n",
    "2. We must not impute them with their means as they are heavily skewed by outliers\n",
    "3. Imputation will be done with median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac05a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing with median\n",
    "median_imputation = SimpleImputer(strategy='median', missing_values=np.nan)\n",
    "df_telco_v1[cols_w_null] = median_imputation.fit_transform(df_telco_v1[cols_w_null])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d90ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ae7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Missing value in all columns\n",
    "round(df_telco_v1.isnull().sum()/len(df_telco_v1),4)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8d9e85",
   "metadata": {},
   "source": [
    "No missing values are observed in any columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c93a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    " df_telco_v1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ac1795",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "1. After null imputation and dropping columns we can see that we are left with 3 months June, July, August in the dataset\n",
    "2. We must create a customer profile from this data to identify high value customer, These customers can be a combination of high ARPU (avg revenue per user) and AON (age on network) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e2c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_dict.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6195d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v1['aon_months'] = df_telco_v1['aon'].apply(lambda x: round((x/365)*12,2))\n",
    "df_telco_v1['aon_months'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d0118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v1['mean_arpu'] = round((df_telco_v1['arpu_6']+df_telco_v1['arpu_7']+df_telco_v1['arpu_8'])/3,2)\n",
    "df_telco_v1['mean_arpu'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a057554",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "1. Age on network in months tells us the least usage was ~5 months an maximum of ~142 months on the network \n",
    "2. mean revenue per user for 3 months is ~280 Units with media ~200 units\n",
    "3. If we combine the aon_months and mean_arpu we can get the avg_customer_Spend and all other columns used for this imputation can be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v1['avg_customer_Spend'] = df_telco_v1['aon_months']*df_telco_v1['mean_arpu']\n",
    "df_telco_v1['avg_customer_Spend'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b676a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v1.drop(columns = ['aon','arpu_6','arpu_7','arpu_8','mean_arpu','aon_months'],axis=1,inplace=True)\n",
    "df_telco_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e5cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "HVC = df_telco_v1['avg_customer_Spend'].quantile(0.65) #High value customers\n",
    "LVC = df_telco_v1['avg_customer_Spend'].quantile(0.2) #low value customers\n",
    "df_telco_v1['customer_value'] = df_telco_v1['avg_customer_Spend'].apply(\n",
    "    lambda x: 'HVC' if x > HVC else ('LVC' if x < LVC else 'MVC')) # MVC- Medium value customers\n",
    "df_telco_v1['customer_value'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af63e2d9",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "1. We now have a customer vaulue and who the telecom operator has to focus on for maximum retention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d724d76",
   "metadata": {},
   "source": [
    "## <font color= Blue> 1.5 Outlier treatment and Visualization (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869179b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype\n",
    "for col in df_telco_v1.columns: \n",
    "    if is_numeric_dtype(df_telco_v1[col]):\n",
    "        plt.title(col)\n",
    "        sns.boxplot(x = df_telco_v1['customer_value'],y=df_telco_v1[col], data = df_telco_v1)\n",
    "        print (\"maximum value for\",col, \"is:\",df_telco_v1[col].max(),\n",
    "               \"\\nminimum value for\",col, \"is:\", df_telco_v1[col].min(),\n",
    "              \"\\nmedian value for\",col, \"is:\", df_telco_v1[col].quantile(0.50))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1590dbd9",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "1. Almost all columns have outliers. \n",
    "2. Outliers will not be removed from all columns e.x columns like SACHET, monthly_3g, spl etc have very low difference between max and median \n",
    "3. Many columns like std recharge, roaming cannot be considered in outlier analysis are they are a cause for revenue and dissatisfaction here may lead to more churn\n",
    "3. Hence outliers are dropped only from columns which have a large mean median difference especially observed in total minutes of usage with combination of onnet and offnet usage. \n",
    "4. values above 99 (assumed) percentile are considered outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_cols=['total_ic_mou_6','total_og_mou_6','onnet_mou_6','offnet_mou_6','total_rech_amt_6','total_ic_mou_7','total_og_mou_7','onnet_mou_7','offnet_mou_7','total_rech_amt_7','total_ic_mou_8','total_og_mou_8','onnet_mou_8','offnet_mou_8','total_rech_amt_8',]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca17ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v1[outlier_cols].describe(percentiles=[0,0.25,0.5,0.75,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca015be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v2=df_telco_v1.copy()\n",
    "for col in outlier_cols:\n",
    "    if is_numeric_dtype(df_telco_v2[col]):\n",
    "        if df_telco_v2[col].max()/df_telco_v2[col].quantile(0.99)>6:\n",
    "            df_telco_v2=df_telco_v2[df_telco_v2[col]<df_telco_v2[col].quantile(0.99)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3ecd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919cb91b",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "1. Among the identified outliers only the very high deviation cases i.e ratio between max and 99percentile is more than 6times only those entries are removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43aef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(((69999-df_telco_v2.shape[0])/69999)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7112ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd720fe",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "1. few key columns were identified and outliers seen in those columns were eliminated resulting trimming dataset by ~3% which is accecptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59796165",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_telco_v2['churn_probability'])\n",
    "plt.title(\"Churn Probability\")\n",
    "plt.show()\n",
    "df_telco_v2['churn_probability'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9f8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# of churn in the filtered dataset\n",
    "round((df_telco_v2['churn_probability'].value_counts()[1]/df_telco_v2['churn_probability'].value_counts()[0])*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53388e00",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "1. A big class imablance is seen between churn (~12%) and active cases (~88%). \n",
    "2. correlation matirx is carried out to understand if there are any visible  patterns correlating with churn and spend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467db777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation plot of whole dataset\n",
    "plt.figure(figsize = (40, 40))\n",
    "sns.heatmap(df_telco_v2.corr(), cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b58f8",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. since the grph is not cler the stdy is carried out in Month-wise order to see if there are any visible patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e33326",
   "metadata": {},
   "outputs": [],
   "source": [
    "June_month =[]\n",
    "July_month = []\n",
    "August_month = []\n",
    "others=[]\n",
    "for i in df_telco_v2.columns:\n",
    "    if '6' in i or 'jun' in i.lower():\n",
    "        June_month.append(i)\n",
    "    elif '7' in i or 'jul' in i.lower():\n",
    "        July_month.append(i)\n",
    "    elif '8' in i or 'aug' in i.lower():\n",
    "        August_month.append(i)\n",
    "    else:\n",
    "        others.append(i)\n",
    "\n",
    "print(June_month, len(June_month))\n",
    "print(July_month, len(July_month))\n",
    "print(August_month, len(August_month))\n",
    "print(others)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c3a1b",
   "metadata": {},
   "source": [
    "Analysis Month wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafdb5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(June_month)):\n",
    "    plt.figure(figsize=(80, 60))\n",
    "    plt.subplot(10,4,i+1)\n",
    "    sns.scatterplot(x = df_telco_v2[June_month[i]], y = 'avg_customer_Spend', data = df_telco_v2, hue='customer_value',style='churn_probability',sizes='churn_probability')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628b094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(July_month)):\n",
    "    plt.figure(figsize=(80, 60))\n",
    "    plt.subplot(10,4,i+1)\n",
    "    sns.scatterplot(x = df_telco_v2[July_month[i]], y = 'avg_customer_Spend', data = df_telco_v2, hue='customer_value',style='churn_probability',sizes='churn_probability')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bded24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(August_month)):\n",
    "    plt.figure(figsize=(80, 60))\n",
    "    plt.subplot(10,4,i+1)\n",
    "    sns.scatterplot(x = df_telco_v2[August_month[i]], y = 'avg_customer_Spend', data = df_telco_v2, hue='customer_value',style='churn_probability',sizes='churn_probability')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c063f",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. Visually more churns ca be seen in high value customers in std_outgoing calls followed by roaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162bd3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(month):\n",
    "    plt.figure(figsize = (40, 40))\n",
    "    sns.heatmap(df_telco_v2[month].corr(), annot=True, cmap=\"YlGnBu\")\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587fc7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "June=June_month+others\n",
    "corr(June)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283f7c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "July=July_month+others\n",
    "corr(July)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd33e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aug=August_month+others\n",
    "corr(Aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea94643",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. We can see small patches where customer spend is positive and aslo the churn posibility is positive although very slightly these variable are important\n",
    "2. Also when the spending is high the  churn probaility is negetive\n",
    "3. from the scatter plot it is also evident that the sd_og is an importaint variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7daced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.countplot(x=\"customer_value\",hue=\"churn_probability\", data=df_telco_v2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7f2adc",
   "metadata": {},
   "source": [
    "##  <font color= Green> Key insights from EDA -\n",
    " - Most customers who are highvalue as per our analysis have a lower churn rate.\n",
    " - Targeting the middle MVC and HVC is benefetial to the company\n",
    " - Mostly the churn probabilty is negetively correlated with most variables which means predictor variables donot show a clear pattern with the target variable\n",
    " - There is a big class imbalance between the most of 88% of the data is active users\n",
    " - only few variables(like std_og, roam etc) gives a clear identification how it is related to revenue and churn\n",
    " - In this dataset June sees highest churn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d06816",
   "metadata": {},
   "source": [
    "## <font color= Purple> Step-2 Data Preperation and modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a9c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v2.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11981619",
   "metadata": {},
   "source": [
    "Handling class imbalance by resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e43bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v2['customer_value'] = df_telco_v2['customer_value'].apply(lambda x:2 if x=='HVC' else (0 if x=='LVC' else 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40370891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Train split\n",
    "y = df_telco_v2.loc[:,'churn_probability']\n",
    "x = df_telco_v2.drop(columns=['churn_probability'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b71329",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=15)\n",
    "X_rsamp, y_rsamp = sm.fit_resample(x, y)\n",
    "print(X_rsamp.shape, y_rsamp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0665d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_telco_v3 = df_telco_v2\n",
    "df_telco_v3 = pd.concat([X_rsamp,y_rsamp],axis=1)\n",
    "df_telco_v3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c77074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v3['customer_value'] = df_telco_v3['customer_value'].apply(lambda x:'HVC' if x==2 else ('LCV' if x==0 else 'MCV'))\n",
    "df_telco_v3['customer_value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e6e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9026d7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.countplot(x=\"customer_value\",hue=\"churn_probability\", data=df_telco_v3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a518db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of churn in the resampled dataset\n",
    "round((df_telco_v3['churn_probability'].value_counts()[1]/df_telco_v3['churn_probability'].value_counts()[0])*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefdec50",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. Now the churn probailities are equally distributed and imbalance is eliminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_encode = pd.get_dummies(df_telco_v3['customer_value'], drop_first=True, prefix='customer_value')\n",
    "df_telco_v3.drop(columns=['customer_value'],axis=1,inplace=True)\n",
    "df_telco_v3 = pd.concat([df_telco_v3,hot_encode],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8773e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ba77c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_telco_v3,train_size=0.80,test_size=0.20,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d520bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.pop('churn_probability')\n",
    "X_train = df_train\n",
    "y_test = df_test.pop('churn_probability')\n",
    "X_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5970ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_telco_v3.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e2e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the train & test datasets\n",
    "scaler = StandardScaler()\n",
    "var=X_train.columns\n",
    "X_train[var] = scaler.fit_transform(X_train[var])\n",
    "X_test[var] = scaler.transform(X_test[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20164683",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95915de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa1a2a8",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. All values seems to be scaled in the same similarly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4047bb41",
   "metadata": {},
   "source": [
    "## <font color= Purple> Step-3 Model Development and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdaab8d",
   "metadata": {},
   "source": [
    "### <font color= Blue> 3.1 Logistic regression is chosen as base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9171bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic = LogisticRegression()\n",
    "model_logistic.fit(X_train, y_train)\n",
    "y_pred_log =  model_logistic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cc79f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(y_test,y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print('The confusion Matrix : \\n',cm)\n",
    "    \n",
    "    TP = cm[1,1] # true positives \n",
    "    TN = cm[0,0] # true negatives\n",
    "    FP = cm[0,1] # false positives\n",
    "    FN = cm[1,0] # false negatives\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_true=y_test,y_pred=y_pred)\n",
    "    recall = TP/(FN+TP)\n",
    "    specificity = TN/(TN+FP)\n",
    "    precision = TP/(FP+TP)\n",
    "\n",
    "    print(\"Accuracy = {:.2f}\".format(accuracy))\n",
    "    print(\"Sensitivity/Recall = {:.2f}\".format(recall))\n",
    "    print(\"Specificity = {:.2f}\".format(specificity))\n",
    "    print(\"Precision = {:.2f}\".format(precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test,y_pred_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c92b0",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. The base model itself is got with very high accuracy. \n",
    "2. all other scores seems to be same at 87%\n",
    "3. Just to get a fair idea VIF is applied and some columns are dropped just to see the influence on the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "## eliminating features by performing VIF\n",
    "\n",
    "def VIF_calc(dataframe):\n",
    "    vif = pd.DataFrame()\n",
    "    vif['Features'] = dataframe.columns\n",
    "    vif['VIF'] = [variance_inflation_factor(dataframe.values, i) for i in range(dataframe.shape[1])]\n",
    "    vif['VIF'] = round(vif['VIF'], 2)\n",
    "    vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "    return vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bbf502",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIF_calc(X_train[var])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0ea9fd",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. VIF shows more than 25 columns with a high value\n",
    "2. This would take many iterations to solve\n",
    "3. Intutively 5 colums with high VIF are randomly dropped just to understand its influence\n",
    "4. the columns which will be dropped are [std_og_mou_7,total_og_mou_7,loc_og_mou_7,loc_og_t2m_mou_7,std_og_t2m_mou_7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c08c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_1=var.drop(['std_og_mou_7','total_og_mou_7','loc_og_mou_7','loc_og_t2m_mou_7','std_og_t2m_mou_7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff2e228",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic.fit(X_train[var_1], y_train)\n",
    "y_pred_log_2 =  model_logistic.predict(X_test[var_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa55f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test,y_pred_log_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bde1dc",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. No change is observed this mean the data variables have high multicolinearity. PCA is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691a088a",
   "metadata": {},
   "source": [
    "### <font color= Blue> 3.2 Decision Tree base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e46f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree = DecisionTreeClassifier(max_depth=3,random_state=100)\n",
    "dec_tree.fit(X_train[var], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f7c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image  \n",
    "from six import StringIO  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus, graphviz\n",
    "\n",
    "def get_graph(classifier):\n",
    "    dot_data = StringIO()  \n",
    "    export_graphviz(classifier, out_file=dot_data, filled=True, rounded=True,\n",
    "                    feature_names=X_train.columns, \n",
    "                    class_names=['Churn', \"Active\"])\n",
    "\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "    return Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_graph(dec_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c351f4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dec_tree_base = dec_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055df4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test,y_pred_dec_tree_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9ca1a",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. decision tree gives same accuray compared to linear regression\n",
    "2. the recall has reduced by 4% and improvement in precision and specificity are seen\n",
    "3. Dimentionality is further reduced by using PCA in the next steps and decision tree classifier is used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20192e8",
   "metadata": {},
   "source": [
    "### <font color= Blue> 3.3 Principal Component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b4308",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70975dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed0dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ada8baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative sum calculation for scree plot to understand variance v/s variables\n",
    "cumsum_vars = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb4dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,6])\n",
    "plt.vlines(x=70, ymax=100, ymin=0, colors=\"g\", linestyles=\"--\")\n",
    "plt.hlines(y=95, xmax=125, xmin=0, colors=\"r\", linestyles=\"-.\")\n",
    "plt.plot(cumsum_vars)\n",
    "plt.ylabel(\"Cumulative variance\")\n",
    "plt.xlabel(\"Variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b28b3ee",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. from the scree plot we can see that 70 variables can represent 95% variation in the data i.e roughly just above half the dataset size.\n",
    "2. This wil be taken forward in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_70_var = PCA(n_components=70,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d209773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_70 = pca_70_var.fit_transform(X_train)\n",
    "X_test_pca_70 = pca_70_var.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_pca_70.shape)\n",
    "print(X_test_pca_70.shape)\n",
    "X_train_pca_70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9139ec9e",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. data set has been reduced to 70 variable array after pca\n",
    "2. Decision tree and random forest are done on this data to calculate the matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58047877",
   "metadata": {},
   "source": [
    "### <font color= Blue> 3.4 Decision tree calssifier with PCA and Hyper tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6eef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree_cla = DecisionTreeClassifier(random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06244f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "params_dec_tree = {\n",
    "    'max_depth': [ 3, 5, 10, 20],\n",
    "    'min_samples_leaf': [5, 10, 20, 50, 100, 1000],\n",
    "    'criterion': [\"gini\", \"entropy\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3697e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=dec_tree_cla, \n",
    "                           param_grid=params_dec_tree, \n",
    "                           cv=4, n_jobs=-1, verbose=1, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa8a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train_pca_70, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ec4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dec_tree = pd.DataFrame(grid_search.cv_results_)\n",
    "score_dec_tree.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61af4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dec_tree.nlargest(5,\"mean_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1f0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06379046",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree_final = DecisionTreeClassifier(criterion='entropy', max_depth=20, min_samples_leaf=5, random_state=100)\n",
    "dec_tree_final.fit(X_train_pca_70, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a387520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dec_tree = dec_tree_final.predict(X_test_pca_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test,y_pred_dec_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb99121",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. The accracy has marginally reduced from 87% to 86%  \n",
    "2. but the other matries have also marginally decreased on all counts except recall.\n",
    "3. Hence is this case the base model decision tree performs slightly better on all counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa948afe",
   "metadata": {},
   "source": [
    "### <font color= Blue> 3.5 Random forest Calssifer with PCA and hyper tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342499a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rf = {\n",
    "    'max_depth': [ 3, 5, 10, 20],\n",
    "    'min_samples_leaf': [5, 10, 20, 50, 100],\n",
    "    'n_estimators': [10, 25, 50, 100,200]\n",
    "}\n",
    "# Instantiate the grid search model\n",
    "rf = RandomForestClassifier(random_state=100)\n",
    "grid_search = GridSearchCV(estimator=rf, \n",
    "                           param_grid=params_rf, \n",
    "                           cv=4, n_jobs=-1, verbose=1, scoring = \"recall\")\n",
    "\n",
    "grid_search.fit(X_train_pca_70, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b921146",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pca_final = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87066f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pca_final.fit(X_train_pca_70,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cbf32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rf_pca_final.predict(X_test_pca_70)\n",
    "get_confusion_matrix(y_test,y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c46bc4",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. Random forest classifier gives better results compared to decision tree on all counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3267609",
   "metadata": {},
   "source": [
    "## <font color= Green> Key Insights Model Development and Evaluation  -\n",
    "1. The base linear regression model was itself very good interms of prediction with 87% accuracy and 87% in recall, precision and specificity\n",
    "2. Here it is to note that logistic regression dataset of telecom was resampled to eliminate class imbalance. without which the accuracy was good but other indeces performed poorly.\n",
    "3. Hence the resampled dataset was used in all model development and hyper tuning exercises\n",
    "4. Also differnt models like Logistic regreesion with VIF, Decision tree (base and tuned), Random Forest (tuned) were performed\n",
    "5. Best results were observed with Random forest classifer with following matrices\n",
    "    Accuracy = 0.92\n",
    "    Sensitivity/Recall = 0.92\n",
    "    Specificity = 0.91\n",
    "    Precision = 0.91\n",
    "this means The classifier detects all the churn cases as churn 92% of the times (Recall) and active cases as active 91% of the time (specificity). Precision also indicates that the cases which were dected churn are actually churn with 91% precision.\n",
    "6. Random forest with hyper tuning of depth 20, sample leaf 5, n estimator 200 is used for final submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08299ece",
   "metadata": {},
   "source": [
    "## <font color= Purple> FINAL PREDICTION (UNSEEN DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c68f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read test csv\n",
    "df_telco_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture customer id\n",
    "Final_pred=pd.DataFrame()\n",
    "Final_pred['id']=df_telco_test.id\n",
    "Final_pred.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b2302",
   "metadata": {},
   "source": [
    "Applying all the Pre-processing on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ef06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1 = df_telco_test.drop(drop_list,axis=1)\n",
    "df_telco_test_v1 = df_telco_test_v1.drop('id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58939412",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1 = df_telco_test_v1.drop(date_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c80f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing with median\n",
    "df_telco_test_v1[cols_w_null] = median_imputation.fit_transform(df_telco_test_v1[cols_w_null])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077b8de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1['aon_months'] = df_telco_test_v1['aon'].apply(lambda x: round((x/365)*12,2))\n",
    "df_telco_test_v1['mean_arpu'] = round((df_telco_test_v1['arpu_6']+df_telco_test_v1['arpu_7']+df_telco_test_v1['arpu_8'])/3,2)\n",
    "df_telco_test_v1['avg_customer_Spend'] = df_telco_test_v1['aon_months']*df_telco_test_v1['mean_arpu']\n",
    "df_telco_test_v1.drop(columns = ['aon','arpu_6','arpu_7','arpu_8','mean_arpu','aon_months'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7997b70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1['customer_value'] = df_telco_test_v1['avg_customer_Spend'].apply(\n",
    "    lambda x: 'HCV' if x > HVC else ('LCV' if x < LVC else 'MCV')) # MVC- Medium value customers\n",
    "df_telco_test_v1['customer_value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2588409",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_encode = pd.get_dummies(df_telco_test_v1['customer_value'], drop_first=True, prefix='customer_value')\n",
    "df_telco_test_v1.drop(columns=['customer_value'],axis=1,inplace=True)\n",
    "df_telco_test_v1 = pd.concat([df_telco_test_v1,hot_encode],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ac7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36edd7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v2=df_telco_test_v1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fd2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "var=df_telco_test_v2.columns\n",
    "df_telco_test_v2[var] = scaler.transform(df_telco_test_v2[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cd4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v2_pca_70 = pca_70_var.transform(df_telco_test_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef384bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_pred['Predictions_decision_tree'] = dec_tree_final.predict(df_telco_test_v2_pca_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_pred.to_csv(\"Unseen_Pred_test_Dec_Tree.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecaf2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_pred=Final_pred.drop('Predictions_decision_tree',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a640a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e87d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_pred['Predictions_Random_Forest'] = rf_pca_final.predict(df_telco_test_v2_pca_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda31c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_pred.to_csv(\"Unseen_Pred_test_Random_forest.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae59ee",
   "metadata": {},
   "source": [
    "## <font color= Green> Suggestions to the company\n",
    " - Most customers who are highvalue as per our analysis have a lower churn rate.\n",
    " - Although highvalue + Middle value customers together have a churn rate of above 60% and Targeting them is benefetial to the company\n",
    " - better offers around roaming and outgoing standard calls will help reducing the churn in this segment\n",
    " - Among the given dataset company can target month of June for releasing new offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d534f732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
